# Virtual Digital Human Expression Driving System


## 项目概述

三个创新点：

| 创新点 | 名称 | 核心技术 |
|--------|------|----------|
| 1 | 双路特征融合 | Audio + Video 跨模态注意力融合 |
| 2 | 扩散模型优化 | DDPM去噪 + 时序一致性约束 |
| 3 | 端到端闭环 | 渲染状态反馈 + 实时校正 |

## 系统架构

```
┌─────────────┐     ┌─────────────┐
│   Audio     │     │   Video     │
│  (MFCC等)   │     │ (关键点等)  │
└──────┬──────┘     └──────┬──────┘
       │                   │
       ▼                   ▼
┌──────────────────────────────────┐
│      双路特征融合 (创新点1)       │
│  Cross-Attention + 动态权重融合   │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│      扩散模型优化 (创新点2)       │
│    DDPM去噪 + 表情细节增强        │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│      闭环校正系统 (创新点3)       │
│   渲染状态反馈 + 实时误差校正      │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│     UE5 MetaHuman (52 BS)        │
│        LiveLink 实时渲染          │
└──────────────────────────────────┘
```

## 安装

```bash
# 克隆项目
git clone https://github.com/your-repo/virtual_ue5.git
cd virtual_ue5

# 安装依赖
pip install -r requirements.txt
```

## 快速开始

### 1. 训练模型

```bash
# 训练基线模型
python train.py --model base_audio --epochs 50

# 训练创新点1：双路融合模型
python train.py --model dual_fusion --epochs 50

# 训练创新点2：扩散模型
python train.py --model diffusion --epochs 50

# 训练创新点3：闭环模型
python train.py --model e2e_loop --epochs 50
```

### 2. 模型对比测试

```bash
# 运行所有模型对比
python test_models.py

# 测试单个模型详情
python test_models.py --model dual_fusion --detail

# 保存结果到文件
python test_models.py --save results.json
```

## 实验结果

### 模型性能对比

在合成数据集上的测试结果：

| 模型 | 参数量 | MSE ↓ | MAE ↓ | 口型误差 ↓ | 抖动 ↓ | FPS ↑ |
|------|--------|-------|-------|------------|--------|-------|
| Base (Audio) | 2.89M | 0.0527 | 0.1895 | 0.0541 | 0.0016 | 141.4 |
| Base (Video) | 3.24M | 0.0549 | 0.1924 | 0.0541 | 0.0001 | 22.0 |
| **创新点1** | 9.33M | 0.0538 | 0.1908 | 0.0543 | 0.0058 | 49.8 |
| **创新点2** | 6.20M | 0.1625 | 0.3426 | 0.1607 | 0.2182 | 5.2 |
| **创新点3** | 5.61M | 0.1235 | 0.2885 | 0.1172 | 0.0004 | 60.4 |

> 注：↓ 表示越低越好，↑ 表示越高越好

### 各创新点优势分析

**创新点1 - 双路特征融合**
- 动态融合权重自适应调整（音频权重 ≈ 0.43，视频权重 ≈ 0.57）
- 跨模态注意力增强特征交互
- 情绪一致性约束

**创新点2 - 扩散模型优化**
- 更大的动态范围（0.908 vs 0.222）
- 细节表情增强
- 支持DDIM加速采样

**创新点3 - 端到端闭环**
- 极低的抖动（0.0004）
- 实时反馈校正
- 支持60+ FPS实时推理

### 推理速度对比

```
序列长度 vs 推理时间 (batch_size=4)

seq_len=25:   dual_fusion  8.2ms   | e2e_loop  6.1ms
seq_len=50:   dual_fusion 20.1ms   | e2e_loop 16.6ms
seq_len=100:  dual_fusion 45.3ms   | e2e_loop 35.2ms
seq_len=200:  dual_fusion 98.7ms   | e2e_loop 72.4ms
```

## 核心模块说明

### 输入输出规格

| 项目 | 维度 | 说明 |
|------|------|------|
| 音频输入 | (B, T, 80) | Mel频谱特征 |
| 视频输入 | (B, T, 1434) | 478个面部关键点 × 3坐标 |
| BlendShape输出 | (B, T, 52) | ARKit 52维表情权重 |
| 头部姿态输出 | (B, T, 4) | 四元数旋转 |

### 损失函数

```python
总损失 = λ1 × BlendShape损失
       + λ2 × 头部姿态损失
       + λ3 × 时序一致性损失
       + λ4 × 口型同步损失
       + λ5 × 情绪一致性损失
```
