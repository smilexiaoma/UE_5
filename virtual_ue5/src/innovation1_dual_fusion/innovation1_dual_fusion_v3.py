"""
åˆ›æ–°ç‚¹1æ”¹è¿›ç‰ˆV3: åŒè·¯ç‰¹å¾èåˆæ¨¡å‹ï¼ˆå®Œæ•´æ”¹è¿›ç‰ˆï¼‰
Audio + Video è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ

æ”¹è¿›ç‚¹ï¼š
1. å¤šå±‚è·¨æ¨¡æ€äº¤äº’
2. è‡ªé€‚åº”é—¨æ§èåˆæœºåˆ¶
3. å¢å¼ºçš„æ—¶é—´å»ºæ¨¡
4. æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
5. æ›´å¼ºçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, List

from ..baseline.base_model import AudioOnlyEncoder, VideoOnlyEncoder, BlendShapeDecoder, HeadPoseDecoder


class CrossModalBlock(nn.Module):
    """è·¨æ¨¡æ€äº¤äº’å— - åŒ…å«åŒå‘æ³¨æ„åŠ›å’ŒFFN"""

    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()

        # éŸ³é¢‘åˆ°è§†é¢‘çš„æ³¨æ„åŠ›
        self.audio_to_video_attn = nn.MultiheadAttention(
            dim, num_heads, batch_first=True, dropout=dropout
        )
        self.audio_norm1 = nn.LayerNorm(dim)

        # è§†é¢‘åˆ°éŸ³é¢‘çš„æ³¨æ„åŠ›
        self.video_to_audio_attn = nn.MultiheadAttention(
            dim, num_heads, batch_first=True, dropout=dropout
        )
        self.video_norm1 = nn.LayerNorm(dim)

        # FFN for audio
        self.audio_ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout),
        )
        self.audio_norm2 = nn.LayerNorm(dim)

        # FFN for video
        self.video_ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout),
        )
        self.video_norm2 = nn.LayerNorm(dim)

    def forward(
        self,
        audio_feat: torch.Tensor,
        video_feat: torch.Tensor,
        return_attention: bool = False
    ) -> tuple:
        """
        Args:
            audio_feat: (B, T, dim)
            video_feat: (B, T, dim)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            audio_out: (B, T, dim)
            video_out: (B, T, dim)
            attn_weights: dict or None
        """
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        audio_attn_out, audio_attn_weights = self.audio_to_video_attn(
            audio_feat, video_feat, video_feat
        )
        audio_feat = self.audio_norm1(audio_feat + audio_attn_out)

        video_attn_out, video_attn_weights = self.video_to_audio_attn(
            video_feat, audio_feat, audio_feat
        )
        video_feat = self.video_norm1(video_feat + video_attn_out)

        # FFN
        audio_feat = self.audio_norm2(audio_feat + self.audio_ffn(audio_feat))
        video_feat = self.video_norm2(video_feat + self.video_ffn(video_feat))

        attn_weights = None
        if return_attention:
            attn_weights = {
                'audio_to_video': audio_attn_weights,
                'video_to_audio': video_attn_weights,
            }

        return audio_feat, video_feat, attn_weights


class AdaptiveFusionGate(nn.Module):
    """è‡ªé€‚åº”èåˆé—¨æ§æœºåˆ¶ - åŠ¨æ€å­¦ä¹ èåˆæƒé‡"""

    def __init__(self, dim: int, dropout: float = 0.1):
        super().__init__()

        self.gate_net = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim, 2),
            nn.Softmax(dim=-1),  # æƒé‡å½’ä¸€åŒ–
        )

    def forward(self, audio_feat: torch.Tensor, video_feat: torch.Tensor) -> torch.Tensor:
        """
        Args:
            audio_feat: (B, T, dim)
            video_feat: (B, T, dim)

        Returns:
            fused: (B, T, dim)
        """
        # æ‹¼æ¥ç‰¹å¾
        concat = torch.cat([audio_feat, video_feat], dim=-1)

        # è®¡ç®—é—¨æ§æƒé‡
        weights = self.gate_net(concat)  # (B, T, 2)

        # åŠ æƒèåˆ
        audio_weight = weights[..., 0:1]  # (B, T, 1)
        video_weight = weights[..., 1:2]  # (B, T, 1)

        fused = audio_weight * audio_feat + video_weight * video_feat

        return fused


class DualFusionModelV3(nn.Module):
    """
    åŒè·¯ç‰¹å¾èåˆæ¨¡å‹ V3 - å®Œæ•´æ”¹è¿›ç‰ˆ

    æ”¹è¿›ç‚¹ï¼š
    1. âœ… å¤šå±‚è·¨æ¨¡æ€äº¤äº’ï¼šå †å å¤šä¸ªCrossModalBlock
    2. âœ… è‡ªé€‚åº”èåˆï¼šä½¿ç”¨é—¨æ§æœºåˆ¶åŠ¨æ€å­¦ä¹ èåˆæƒé‡
    3. âœ… å¢å¼ºæ—¶é—´å»ºæ¨¡ï¼šåŒå±‚LSTM + Temporal Attention
    4. âœ… æ³¨æ„åŠ›å¯è§†åŒ–ï¼šä¿å­˜æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡
    5. âœ… æ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼šæ›´æ·±çš„ç½‘ç»œç»“æ„
    """

    def __init__(
        self,
        audio_dim: int = 80,
        video_dim: int = 478 * 3,
        hidden_dim: int = 256,
        blendshape_dim: int = 52,
        num_layers: int = 2,
        num_cross_modal_layers: int = 2,  # è·¨æ¨¡æ€äº¤äº’å±‚æ•°
        dropout: float = 0.1,
        use_adaptive_fusion: bool = True,  # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”èåˆ
    ):
        super().__init__()

        self.num_cross_modal_layers = num_cross_modal_layers
        self.use_adaptive_fusion = use_adaptive_fusion

        # éŸ³é¢‘å’Œè§†é¢‘ç¼–ç å™¨
        self.audio_encoder = AudioOnlyEncoder(audio_dim, hidden_dim, num_layers, dropout)
        self.video_encoder = VideoOnlyEncoder(video_dim, hidden_dim, num_layers, dropout)

        encoder_dim = hidden_dim * 2  # LSTMæ˜¯åŒå‘çš„

        # ğŸ†• å¤šå±‚è·¨æ¨¡æ€äº¤äº’
        self.cross_modal_layers = nn.ModuleList([
            CrossModalBlock(encoder_dim, num_heads=8, dropout=dropout)
            for _ in range(num_cross_modal_layers)
        ])

        # ğŸ†• è‡ªé€‚åº”èåˆé—¨æ§
        if use_adaptive_fusion:
            self.fusion_gate = AdaptiveFusionGate(encoder_dim, dropout)
            fusion_dim = encoder_dim
        else:
            # ä¼ ç»Ÿæ‹¼æ¥èåˆ
            fusion_dim = encoder_dim * 2

        # èåˆåçš„å¤„ç†
        self.post_fusion = nn.Sequential(
            nn.Linear(fusion_dim, encoder_dim * 2),
            nn.LayerNorm(encoder_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),

            nn.Linear(encoder_dim * 2, encoder_dim),
            nn.LayerNorm(encoder_dim),
        )

        # ğŸ†• å¢å¼ºçš„æ—¶é—´å»ºæ¨¡
        self.temporal_lstm = nn.LSTM(
            input_size=encoder_dim,
            hidden_size=encoder_dim // 2,
            num_layers=2,  # åŒå±‚LSTM
            batch_first=True,
            bidirectional=True,
            dropout=dropout,
        )
        self.temporal_norm = nn.LayerNorm(encoder_dim)

        # æ—¶é—´æ³¨æ„åŠ›ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
        self.temporal_attention = nn.MultiheadAttention(
            encoder_dim, num_heads=8, batch_first=True, dropout=dropout
        )
        self.temporal_attn_norm = nn.LayerNorm(encoder_dim)

        # è§£ç å™¨
        self.blendshape_decoder = BlendShapeDecoder(encoder_dim, hidden_dim, blendshape_dim, dropout)
        self.head_pose_decoder = HeadPoseDecoder(encoder_dim, hidden_dim // 2)

    def forward(
        self,
        audio: torch.Tensor,
        video: torch.Tensor,
        return_attention: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            audio: (B, T, 80) éŸ³é¢‘ç‰¹å¾
            video: (B, T, 1434) è§†é¢‘ç‰¹å¾
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–

        Returns:
            dict: {
                'blendshapes': (B, T, 52),
                'head_pose': (B, T, 4),
                'cross_modal_attention': List[dict] [å¯é€‰],
                'temporal_attention': (B, num_heads, T, T) [å¯é€‰],
                'fusion_weights': (B, T, 2) [å¯é€‰, ä»…adaptive fusion],
            }
        """
        # 1. ç¼–ç 
        audio_feat = self.audio_encoder(audio)  # (B, T, encoder_dim)
        video_feat = self.video_encoder(video)  # (B, T, encoder_dim)

        # 2. ğŸ†• å¤šå±‚è·¨æ¨¡æ€äº¤äº’
        cross_modal_attentions = []
        for i, cross_modal_layer in enumerate(self.cross_modal_layers):
            audio_feat, video_feat, attn_weights = cross_modal_layer(
                audio_feat, video_feat, return_attention=return_attention
            )
            if return_attention and attn_weights is not None:
                cross_modal_attentions.append(attn_weights)

        # 3. ğŸ†• è‡ªé€‚åº”èåˆ
        if self.use_adaptive_fusion:
            fused = self.fusion_gate(audio_feat, video_feat)
            # ä¿å­˜èåˆæƒé‡ç”¨äºåˆ†æ
            if return_attention:
                # é‡æ–°è®¡ç®—é—¨æ§æƒé‡ç”¨äºè¿”å›
                concat = torch.cat([audio_feat, video_feat], dim=-1)
                fusion_weights = self.fusion_gate.gate_net(concat)
            else:
                fusion_weights = None
        else:
            fused = torch.cat([audio_feat, video_feat], dim=-1)
            fusion_weights = None

        # 4. èåˆåå¤„ç†
        fused = self.post_fusion(fused)  # (B, T, encoder_dim)

        # 5. ğŸ†• å¢å¼ºçš„æ—¶é—´å»ºæ¨¡
        # 5.1 åŒå±‚LSTM
        fused_temporal, _ = self.temporal_lstm(fused)
        fused = self.temporal_norm(fused + fused_temporal)

        # 5.2 æ—¶é—´è‡ªæ³¨æ„åŠ›
        temporal_attn_out, temporal_attn_weights = self.temporal_attention(
            fused, fused, fused
        )
        fused = self.temporal_attn_norm(fused + temporal_attn_out)

        # 6. è§£ç 
        blendshapes = self.blendshape_decoder(fused)
        head_pose = self.head_pose_decoder(fused)

        outputs = {
            'blendshapes': blendshapes,
            'head_pose': head_pose,
        }

        # ğŸ†• è¿”å›æ³¨æ„åŠ›æƒé‡å’Œèåˆæƒé‡ï¼ˆç”¨äºå¯è§†åŒ–å’Œåˆ†æï¼‰
        if return_attention:
            if len(cross_modal_attentions) > 0:
                outputs['cross_modal_attention'] = cross_modal_attentions
            outputs['temporal_attention'] = temporal_attn_weights
            if fusion_weights is not None:
                outputs['fusion_weights'] = fusion_weights

        return outputs

    def compute_loss(
        self,
        outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤±å‡½æ•°

        Args:
            outputs: æ¨¡å‹è¾“å‡º
            targets: ç›®æ ‡å€¼
        """
        losses = {}

        # BlendShapeæŸå¤±
        losses['blendshape_loss'] = F.mse_loss(outputs['blendshapes'], targets['blendshapes'])

        # å¤´éƒ¨å§¿æ€æŸå¤±
        losses['head_pose_loss'] = F.mse_loss(outputs['head_pose'], targets['head_pose'])

        # æ—¶é—´å¹³æ»‘åº¦æŸå¤±
        pred_diff = outputs['blendshapes'][:, 1:] - outputs['blendshapes'][:, :-1]
        losses['temporal_loss'] = (pred_diff ** 2).mean()

        # æ€»æŸå¤±
        losses['total_loss'] = (
            losses['blendshape_loss'] +
            0.5 * losses['head_pose_loss'] +
            0.1 * losses['temporal_loss']
        )

        return losses


def create_dual_fusion_model_v3(config: Optional[Dict] = None) -> DualFusionModelV3:
    """åˆ›å»ºV3æ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    default_config = {
        'audio_dim': 80,
        'video_dim': 478 * 3,
        'hidden_dim': 256,
        'blendshape_dim': 52,
        'num_layers': 2,
        'num_cross_modal_layers': 2,
        'dropout': 0.1,
        'use_adaptive_fusion': True,
    }
    if config:
        default_config.update(config)
    return DualFusionModelV3(**default_config)


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    print("\n" + "="*60)
    print("Testing DualFusionModelV3")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    model = create_dual_fusion_model_v3().to(device)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size, seq_len = 4, 100
    audio_input = torch.randn(batch_size, seq_len, 80).to(device)
    video_input = torch.randn(batch_size, seq_len, 478 * 3).to(device)

    # å‰å‘ä¼ æ’­
    print("\n1. Forward pass without attention:")
    outputs = model(audio=audio_input, video=video_input)
    print(f"   BlendShapes: {outputs['blendshapes'].shape}")
    print(f"   Head pose: {outputs['head_pose'].shape}")

    # å‰å‘ä¼ æ’­ï¼ˆå¸¦æ³¨æ„åŠ›æƒé‡ï¼‰
    print("\n2. Forward pass with attention:")
    outputs_with_attn = model(audio=audio_input, video=video_input, return_attention=True)
    print(f"   BlendShapes: {outputs_with_attn['blendshapes'].shape}")
    print(f"   Head pose: {outputs_with_attn['head_pose'].shape}")
    if 'cross_modal_attention' in outputs_with_attn:
        print(f"   Cross-modal attention layers: {len(outputs_with_attn['cross_modal_attention'])}")
    if 'temporal_attention' in outputs_with_attn:
        print(f"   Temporal attention: {outputs_with_attn['temporal_attention'].shape}")
    if 'fusion_weights' in outputs_with_attn:
        print(f"   Fusion weights: {outputs_with_attn['fusion_weights'].shape}")
        # æ‰“å°å¹³å‡èåˆæƒé‡
        avg_weights = outputs_with_attn['fusion_weights'].mean(dim=[0, 1])
        print(f"   Avg audio weight: {avg_weights[0]:.4f}, Avg video weight: {avg_weights[1]:.4f}")

    # æŸå¤±è®¡ç®—
    print("\n3. Loss computation:")
    targets = {
        'blendshapes': torch.rand(batch_size, seq_len, 52).to(device),
        'head_pose': F.normalize(torch.randn(batch_size, seq_len, 4), dim=-1).to(device),
    }
    losses = model.compute_loss(outputs, targets)
    for name, value in losses.items():
        print(f"   {name}: {value.item():.6f}")

    # æ¨¡å‹å‚æ•°
    print("\n4. Model info:")
    num_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {num_params:,}")
    print(f"   Size: {num_params / 1e6:.2f}M")

    print("\n" + "="*60)
    print("âœ“ All tests passed!")
    print("="*60)
