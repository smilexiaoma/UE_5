"""
åˆ›æ–°ç‚¹1æ”¹è¿›ç‰ˆV2: åŒè·¯ç‰¹å¾èåˆæ¨¡å‹ï¼ˆæœ€å°æ”¹åŠ¨ç‰ˆï¼‰
Audio + Video è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ

æ”¹è¿›ç‚¹ï¼š
1. æ·»åŠ FFNå±‚å¢å¼ºç‰¹å¾è¡¨è¾¾
2. ä¿ç•™åŸå§‹ç‰¹å¾é¿å…ä¿¡æ¯ä¸¢å¤±
3. å¢å¼ºèåˆå±‚ï¼ˆå¤šå±‚MLPï¼‰
4. æ·»åŠ æ—¶é—´å»ºæ¨¡å±‚
5. æ”¯æŒæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional

from ..baseline.base_model import AudioOnlyEncoder, VideoOnlyEncoder, BlendShapeDecoder, HeadPoseDecoder


class CrossModalAttentionV2(nn.Module):
    """æ”¹è¿›çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ - æ”¯æŒè¿”å›æ³¨æ„åŠ›æƒé‡"""

    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=dropout)
        self.norm = nn.LayerNorm(dim)

    def forward(
        self,
        query: torch.Tensor,
        key_value: torch.Tensor,
        return_attention: bool = False
    ) -> tuple:
        """
        Args:
            query: (B, T, dim)
            key_value: (B, T, dim)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            output: (B, T, dim)
            attn_weights: (B, num_heads, T, T) or None
        """
        attn_out, attn_weights = self.attention(query, key_value, key_value)
        output = self.norm(query + attn_out)

        if return_attention:
            return output, attn_weights
        return output, None


class FeedForwardNetwork(nn.Module):
    """å‰é¦ˆç½‘ç»œå±‚ - å¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›"""

    def __init__(self, dim: int, hidden_dim: int = None, dropout: float = 0.1):
        super().__init__()
        if hidden_dim is None:
            hidden_dim = dim * 4

        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout),
        )
        self.norm = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.norm(x + self.net(x))


class DualFusionModelV2(nn.Module):
    """
    åŒè·¯ç‰¹å¾èåˆæ¨¡å‹ V2 - æœ€å°æ”¹åŠ¨ç‰ˆ

    æ”¹è¿›ç‚¹ï¼š
    1. âœ… æ·»åŠ FFNå±‚ï¼šåœ¨è·¨æ¨¡æ€æ³¨æ„åŠ›åæ·»åŠ å‰é¦ˆç½‘ç»œ
    2. âœ… ä¿ç•™åŸå§‹ç‰¹å¾ï¼šèåˆæ—¶ä¿ç•™audio_featå’Œvideo_feat
    3. âœ… å¢å¼ºèåˆå±‚ï¼šä»å•å±‚Linearæ”¹ä¸ºå¤šå±‚MLP
    4. âœ… æ·»åŠ æ—¶é—´å»ºæ¨¡ï¼šèåˆåæ·»åŠ LSTMå±‚
    5. âœ… æ³¨æ„åŠ›å¯è§†åŒ–ï¼šæ”¯æŒè¿”å›æ³¨æ„åŠ›æƒé‡
    """

    def __init__(
        self,
        audio_dim: int = 80,
        video_dim: int = 478 * 3,
        hidden_dim: int = 256,
        blendshape_dim: int = 52,
        num_layers: int = 2,
        dropout: float = 0.1,
        use_temporal_lstm: bool = True,  # æ˜¯å¦ä½¿ç”¨æ—¶é—´å»ºæ¨¡
    ):
        super().__init__()

        self.use_temporal_lstm = use_temporal_lstm

        # éŸ³é¢‘å’Œè§†é¢‘ç¼–ç å™¨
        self.audio_encoder = AudioOnlyEncoder(audio_dim, hidden_dim, num_layers, dropout)
        self.video_encoder = VideoOnlyEncoder(video_dim, hidden_dim, num_layers, dropout)

        encoder_dim = hidden_dim * 2  # LSTMæ˜¯åŒå‘çš„

        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.audio_to_video_attn = CrossModalAttentionV2(encoder_dim, num_heads=8, dropout=dropout)
        self.video_to_audio_attn = CrossModalAttentionV2(encoder_dim, num_heads=8, dropout=dropout)

        # ğŸ†• æ”¹è¿›1: æ·»åŠ FFNå±‚
        self.audio_ffn = FeedForwardNetwork(encoder_dim, encoder_dim * 2, dropout)
        self.video_ffn = FeedForwardNetwork(encoder_dim, encoder_dim * 2, dropout)

        # ğŸ†• æ”¹è¿›2: èåˆæ—¶ä¿ç•™åŸå§‹ç‰¹å¾ï¼Œè¾“å…¥ç»´åº¦å˜ä¸º encoder_dim * 4
        fusion_input_dim = encoder_dim * 4

        # ğŸ†• æ”¹è¿›3: å¢å¼ºèåˆå±‚ï¼ˆå¤šå±‚MLPï¼‰
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, fusion_input_dim),
            nn.LayerNorm(fusion_input_dim),
            nn.GELU(),
            nn.Dropout(dropout),

            nn.Linear(fusion_input_dim, encoder_dim * 2),
            nn.LayerNorm(encoder_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),

            nn.Linear(encoder_dim * 2, encoder_dim),
            nn.LayerNorm(encoder_dim),
        )

        # ğŸ†• æ”¹è¿›4: æ·»åŠ æ—¶é—´å»ºæ¨¡å±‚
        if use_temporal_lstm:
            self.temporal_lstm = nn.LSTM(
                input_size=encoder_dim,
                hidden_size=encoder_dim // 2,
                num_layers=1,
                batch_first=True,
                bidirectional=True,
                dropout=0,
            )
            self.temporal_norm = nn.LayerNorm(encoder_dim)

        # è§£ç å™¨
        self.blendshape_decoder = BlendShapeDecoder(encoder_dim, hidden_dim, blendshape_dim, dropout)
        self.head_pose_decoder = HeadPoseDecoder(encoder_dim, hidden_dim // 2)

    def forward(
        self,
        audio: torch.Tensor,
        video: torch.Tensor,
        return_attention: bool = False  # ğŸ†• æ”¹è¿›5: æ”¯æŒè¿”å›æ³¨æ„åŠ›æƒé‡
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            audio: (B, T, 80) éŸ³é¢‘ç‰¹å¾
            video: (B, T, 1434) è§†é¢‘ç‰¹å¾
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–

        Returns:
            dict: {
                'blendshapes': (B, T, 52),
                'head_pose': (B, T, 4),
                'audio_attention': (B, num_heads, T, T) [å¯é€‰],
                'video_attention': (B, num_heads, T, T) [å¯é€‰],
            }
        """
        # 1. ç¼–ç 
        audio_feat = self.audio_encoder(audio)  # (B, T, encoder_dim)
        video_feat = self.video_encoder(video)  # (B, T, encoder_dim)

        # 2. è·¨æ¨¡æ€æ³¨æ„åŠ› + FFN
        audio_enhanced, audio_attn = self.audio_to_video_attn(
            audio_feat, video_feat, return_attention=return_attention
        )
        audio_enhanced = self.audio_ffn(audio_enhanced)  # ğŸ†• FFNå¢å¼º

        video_enhanced, video_attn = self.video_to_audio_attn(
            video_feat, audio_feat, return_attention=return_attention
        )
        video_enhanced = self.video_ffn(video_enhanced)  # ğŸ†• FFNå¢å¼º

        # 3. èåˆï¼ˆğŸ†• ä¿ç•™åŸå§‹ç‰¹å¾ï¼‰
        fused = torch.cat([
            audio_feat,      # åŸå§‹éŸ³é¢‘ç‰¹å¾
            video_feat,      # åŸå§‹è§†é¢‘ç‰¹å¾
            audio_enhanced,  # å¢å¼ºåçš„éŸ³é¢‘ç‰¹å¾
            video_enhanced,  # å¢å¼ºåçš„è§†é¢‘ç‰¹å¾
        ], dim=-1)  # (B, T, encoder_dim * 4)

        fused = self.fusion(fused)  # (B, T, encoder_dim)

        # 4. ğŸ†• æ—¶é—´å»ºæ¨¡
        if self.use_temporal_lstm:
            fused_temporal, _ = self.temporal_lstm(fused)
            fused = self.temporal_norm(fused + fused_temporal)  # æ®‹å·®è¿æ¥

        # 5. è§£ç 
        blendshapes = self.blendshape_decoder(fused)
        head_pose = self.head_pose_decoder(fused)

        outputs = {
            'blendshapes': blendshapes,
            'head_pose': head_pose,
        }

        # ğŸ†• è¿”å›æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        if return_attention and audio_attn is not None:
            outputs['audio_attention'] = audio_attn
            outputs['video_attention'] = video_attn

        return outputs

    def compute_loss(
        self,
        outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤±å‡½æ•°

        Args:
            outputs: æ¨¡å‹è¾“å‡º
            targets: ç›®æ ‡å€¼
        """
        losses = {}

        # BlendShapeæŸå¤±
        losses['blendshape_loss'] = F.mse_loss(outputs['blendshapes'], targets['blendshapes'])

        # å¤´éƒ¨å§¿æ€æŸå¤±
        losses['head_pose_loss'] = F.mse_loss(outputs['head_pose'], targets['head_pose'])

        # æ—¶é—´å¹³æ»‘åº¦æŸå¤±
        pred_diff = outputs['blendshapes'][:, 1:] - outputs['blendshapes'][:, :-1]
        losses['temporal_loss'] = (pred_diff ** 2).mean()

        # æ€»æŸå¤±
        losses['total_loss'] = (
            losses['blendshape_loss'] +
            0.5 * losses['head_pose_loss'] +
            0.1 * losses['temporal_loss']
        )

        return losses


def create_dual_fusion_model_v2(config: Optional[Dict] = None) -> DualFusionModelV2:
    """åˆ›å»ºV2æ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    default_config = {
        'audio_dim': 80,
        'video_dim': 478 * 3,
        'hidden_dim': 256,
        'blendshape_dim': 52,
        'num_layers': 2,
        'dropout': 0.1,
        'use_temporal_lstm': True,
    }
    if config:
        default_config.update(config)
    return DualFusionModelV2(**default_config)


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    print("\n" + "="*60)
    print("Testing DualFusionModelV2")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    model = create_dual_fusion_model_v2().to(device)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size, seq_len = 4, 100
    audio_input = torch.randn(batch_size, seq_len, 80).to(device)
    video_input = torch.randn(batch_size, seq_len, 478 * 3).to(device)

    # å‰å‘ä¼ æ’­
    print("\n1. Forward pass without attention:")
    outputs = model(audio=audio_input, video=video_input)
    print(f"   BlendShapes: {outputs['blendshapes'].shape}")
    print(f"   Head pose: {outputs['head_pose'].shape}")

    # å‰å‘ä¼ æ’­ï¼ˆå¸¦æ³¨æ„åŠ›æƒé‡ï¼‰
    print("\n2. Forward pass with attention:")
    outputs_with_attn = model(audio=audio_input, video=video_input, return_attention=True)
    print(f"   BlendShapes: {outputs_with_attn['blendshapes'].shape}")
    print(f"   Head pose: {outputs_with_attn['head_pose'].shape}")
    if 'audio_attention' in outputs_with_attn:
        print(f"   Audio attention: {outputs_with_attn['audio_attention'].shape}")
        print(f"   Video attention: {outputs_with_attn['video_attention'].shape}")

    # æŸå¤±è®¡ç®—
    print("\n3. Loss computation:")
    targets = {
        'blendshapes': torch.rand(batch_size, seq_len, 52).to(device),
        'head_pose': F.normalize(torch.randn(batch_size, seq_len, 4), dim=-1).to(device),
    }
    losses = model.compute_loss(outputs, targets)
    for name, value in losses.items():
        print(f"   {name}: {value.item():.6f}")

    # æ¨¡å‹å‚æ•°
    print("\n4. Model info:")
    num_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {num_params:,}")
    print(f"   Size: {num_params / 1e6:.2f}M")

    print("\n" + "="*60)
    print("âœ“ All tests passed!")
    print("="*60)
