"""
è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–
"""

import os
import sys
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.baseline.base_model import create_base_model
from src.innovation1_dual_fusion.innovation1_dual_fusion import create_dual_fusion_model
from src.innovation2_diffusion.innovation2_diffusion import create_diffusion_model
from src.innovation3_e2e_loop.innovation3_e2e_loop import create_e2e_loop_model
from utils.common import get_device
from utils.ravdess_dataset import RAVDESSDataset


def create_model(model_name: str, config=None):
    """åˆ›å»ºæ¨¡å‹"""
    if model_name == 'base_audio':
        return create_base_model(mode='audio', config=config)
    elif model_name == 'base_video':
        return create_base_model(mode='video', config=config)
    elif model_name == 'dual_fusion':
        return create_dual_fusion_model(config=config)
    elif model_name == 'diffusion':
        return create_diffusion_model(config=config)
    elif model_name == 'e2e_loop':
        return create_e2e_loop_model(config=config)
    else:
        raise ValueError(f"Unknown model: {model_name}")


def evaluate_model(model, dataloader, device, model_name):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()

    # ç”¨äºå­˜å‚¨æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼
    all_pred_blendshapes = []
    all_true_blendshapes = []
    all_pred_head_pose = []
    all_true_head_pose = []

    # æŸå¤±ç»Ÿè®¡
    loss_stats = {
        'total_loss': [],
        'blendshape_loss': [],
        'head_pose_loss': [],
        'temporal_loss': [],
    }

    print("å¼€å§‹è¯„ä¼°...")
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦"):
            audio = batch['audio'].to(device)
            video = batch['video'].to(device)
            blendshapes = batch['blendshapes'].to(device)
            head_pose = batch['head_pose'].to(device)

            targets = {
                'blendshapes': blendshapes,
                'head_pose': head_pose,
            }

            # å‰å‘ä¼ æ’­
            if model_name == 'base_audio':
                outputs = model(audio=audio)
            elif model_name == 'base_video':
                outputs = model(video=video)
            elif model_name == 'dual_fusion':
                outputs = model(audio=audio, video=video)
            elif model_name == 'diffusion':
                outputs = model(audio=audio, video=video, blendshapes=blendshapes)
            elif model_name == 'e2e_loop':
                outputs = model(audio=audio, video=video)
            else:
                raise ValueError(f"Unknown model: {model_name}")

            # è®¡ç®—æŸå¤±
            losses = model.compute_loss(outputs, targets)

            # è®°å½•æŸå¤±
            for key in loss_stats.keys():
                if key in losses and isinstance(losses[key], torch.Tensor):
                    loss_stats[key].append(losses[key].item())

            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            all_pred_blendshapes.append(outputs['blendshapes'].cpu().numpy())
            all_true_blendshapes.append(blendshapes.cpu().numpy())
            all_pred_head_pose.append(outputs['head_pose'].cpu().numpy())
            all_true_head_pose.append(head_pose.cpu().numpy())

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_pred_blendshapes = np.concatenate(all_pred_blendshapes, axis=0)
    all_true_blendshapes = np.concatenate(all_true_blendshapes, axis=0)
    all_pred_head_pose = np.concatenate(all_pred_head_pose, axis=0)
    all_true_head_pose = np.concatenate(all_true_head_pose, axis=0)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    results = {}

    # å¹³å‡æŸå¤±
    for key, values in loss_stats.items():
        if values:
            results[f'avg_{key}'] = np.mean(values)

    # Blendshape è¯„ä¼°æŒ‡æ ‡
    blendshape_mae = np.mean(np.abs(all_pred_blendshapes - all_true_blendshapes))
    blendshape_mse = np.mean((all_pred_blendshapes - all_true_blendshapes) ** 2)
    blendshape_rmse = np.sqrt(blendshape_mse)

    results['blendshape_mae'] = blendshape_mae
    results['blendshape_mse'] = blendshape_mse
    results['blendshape_rmse'] = blendshape_rmse

    # Head Pose è¯„ä¼°æŒ‡æ ‡
    head_pose_mae = np.mean(np.abs(all_pred_head_pose - all_true_head_pose))
    head_pose_mse = np.mean((all_pred_head_pose - all_true_head_pose) ** 2)
    head_pose_rmse = np.sqrt(head_pose_mse)

    results['head_pose_mae'] = head_pose_mae
    results['head_pose_mse'] = head_pose_mse
    results['head_pose_rmse'] = head_pose_rmse

    # è®¡ç®—æ¯ä¸ª blendshape çš„å¹³å‡è¯¯å·®
    per_blendshape_mae = np.mean(np.abs(all_pred_blendshapes - all_true_blendshapes), axis=(0, 1))
    results['per_blendshape_mae'] = per_blendshape_mae.tolist()

    return results, all_pred_blendshapes, all_true_blendshapes, all_pred_head_pose, all_true_head_pose


def print_results(results, model_name):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "=" * 80)
    print(f"æ¨¡å‹è¯„ä¼°ç»“æœ: {model_name}")
    print("=" * 80)

    print("\nğŸ“Š æŸå¤±æŒ‡æ ‡:")
    print(f"  æ€»æŸå¤± (Total Loss):              {results.get('avg_total_loss', 0):.6f}")
    if 'avg_blendshape_loss' in results:
        print(f"  BlendshapeæŸå¤±:                   {results['avg_blendshape_loss']:.6f}")
    if 'avg_head_pose_loss' in results:
        print(f"  å¤´éƒ¨å§¿æ€æŸå¤±:                     {results['avg_head_pose_loss']:.6f}")
    if 'avg_temporal_loss' in results:
        print(f"  æ—¶åºä¸€è‡´æ€§æŸå¤±:                   {results['avg_temporal_loss']:.6f}")

    print("\nğŸ“ˆ Blendshape è¯„ä¼°æŒ‡æ ‡:")
    print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):               {results['blendshape_mae']:.6f}")
    print(f"  å‡æ–¹è¯¯å·® (MSE):                   {results['blendshape_mse']:.6f}")
    print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):                {results['blendshape_rmse']:.6f}")

    print("\nğŸ¯ å¤´éƒ¨å§¿æ€è¯„ä¼°æŒ‡æ ‡:")
    print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):               {results['head_pose_mae']:.6f}")
    print(f"  å‡æ–¹è¯¯å·® (MSE):                   {results['head_pose_mse']:.6f}")
    print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):                {results['head_pose_rmse']:.6f}")

    print("\n" + "=" * 80)


def save_visualizations(results, pred_blendshapes, true_blendshapes, save_dir):
    """ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    viz_dir = Path(save_dir) / 'visualizations'
    viz_dir.mkdir(exist_ok=True)

    # 1. ç»˜åˆ¶æ¯ä¸ª blendshape çš„è¯¯å·®åˆ†å¸ƒ
    per_blendshape_mae = results['per_blendshape_mae']

    plt.figure(figsize=(15, 6))
    plt.bar(range(len(per_blendshape_mae)), per_blendshape_mae, color='steelblue', alpha=0.7)
    plt.xlabel('Blendshape Index', fontsize=12)
    plt.ylabel('Mean Absolute Error', fontsize=12)
    plt.title('Per-Blendshape MAE Distribution', fontsize=14, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(viz_dir / 'per_blendshape_error.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 2. ç»˜åˆ¶é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾ï¼ˆé‡‡æ ·éƒ¨åˆ†æ•°æ®ï¼‰
    sample_size = min(5000, pred_blendshapes.size)
    sample_indices = np.random.choice(pred_blendshapes.size, sample_size, replace=False)

    pred_sample = pred_blendshapes.flatten()[sample_indices]
    true_sample = true_blendshapes.flatten()[sample_indices]

    plt.figure(figsize=(10, 10))
    plt.scatter(true_sample, pred_sample, alpha=0.3, s=1, color='steelblue')

    # æ·»åŠ ç†æƒ³å¯¹è§’çº¿
    min_val = min(true_sample.min(), pred_sample.min())
    max_val = max(true_sample.max(), pred_sample.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideal')

    plt.xlabel('True Blendshape Values', fontsize=12)
    plt.ylabel('Predicted Blendshape Values', fontsize=12)
    plt.title('Predicted vs True Blendshapes', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(viz_dir / 'pred_vs_true_scatter.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 3. ç»˜åˆ¶è¯¯å·®ç›´æ–¹å›¾
    errors = (pred_blendshapes - true_blendshapes).flatten()

    plt.figure(figsize=(12, 6))
    plt.hist(errors, bins=100, color='steelblue', alpha=0.7, edgecolor='black')
    plt.xlabel('Prediction Error', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.title('Error Distribution', fontsize=14, fontweight='bold')
    plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    plt.legend(fontsize=10)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(viz_dir / 'error_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"\nâœ“ å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {viz_dir}")


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹")
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='/root/autodl-tmp/digietal_data/1188976',
                       help='RAVDESSæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤§å°')
    parser.add_argument('--save_results', action='store_true', help='ä¿å­˜è¯„ä¼°ç»“æœ')

    args = parser.parse_args()

    # åŠ è½½æ¨¡å‹é…ç½®
    model_dir = Path(args.model_path).parent
    config_path = model_dir / 'config.json'

    if not config_path.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}")
        return

    with open(config_path, 'r') as f:
        config = json.load(f)

    print(f"åŠ è½½æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {config['model']}")
    print(f"  Actor: {config.get('actor', 'å…¨éƒ¨')}")

    # åˆ›å»ºæ•°æ®é›†
    print(f"\nåŠ è½½æ•°æ®é›†...")
    actors = [config['actor']] if config.get('actor') else None

    dataset = RAVDESSDataset(
        data_dir=args.data_dir,
        actors=actors,
        seq_len=config.get('seq_len', 100),
        use_cache=True,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
    )

    print(f"æ•°æ®é›†æ ·æœ¬æ•°: {len(dataset)}")

    # åˆ›å»ºæ¨¡å‹
    device = get_device()
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    model = create_model(config['model'])
    model = model.to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    print(f"åŠ è½½æ¨¡å‹æƒé‡: {args.model_path}")
    checkpoint = torch.load(args.model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])

    print(f"æ¨¡å‹epoch: {checkpoint.get('epoch', 'Unknown')}")
    print(f"è®­ç»ƒéªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'Unknown')}")

    # è¯„ä¼°æ¨¡å‹
    results, pred_bs, true_bs, pred_hp, true_hp = evaluate_model(
        model, dataloader, device, config['model']
    )

    # æ‰“å°ç»“æœ
    print_results(results, config['model'])

    # ä¿å­˜ç»“æœ
    if args.save_results:
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        results_serializable = {}
        for key, value in results.items():
            if isinstance(value, (np.floating, np.integer)):
                results_serializable[key] = float(value)
            elif isinstance(value, np.ndarray):
                results_serializable[key] = value.tolist()
            elif isinstance(value, list):
                results_serializable[key] = [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in value]
            else:
                results_serializable[key] = value

        results_path = model_dir / 'evaluation_results.json'
        with open(results_path, 'w') as f:
            json.dump(results_serializable, f, indent=2)
        print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_path}")

        # ä¿å­˜å¯è§†åŒ–
        save_visualizations(results, pred_bs, true_bs, model_dir)


if __name__ == '__main__':
    main()
